{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/\"\n",
    "data_file = data_path + \"names.txt\"\n",
    "\n",
    "words = open(data_file, 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map characters to indices (\"string-to-index\")\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "s_to_i = {s: i+1 for i, s in enumerate(chars)}\n",
    "\n",
    "# Include a special character -  we use '.' instead of having both <S> and <E>\n",
    "s_to_i['.'] = 0\n",
    "\n",
    "# Create a dictionary to map indices to characters (\"index-to-string\")\n",
    "i_to_s = {i: ch for ch, i in s_to_i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training set of bigrams (x, y)\n",
    "# Currently limited to just the first 5 characters, \".emma\"\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = s_to_i[ch1]\n",
    "        ix2 = s_to_i[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e 0 5\n",
      "e m 5 13\n",
      "m m 13 13\n",
      "m a 13 1\n",
      "a . 1 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(i_to_s[xs[i].item()], i_to_s[ys[i].item()], xs[i].item(), ys[i].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(s_to_i)\n",
    "\n",
    "x_enc = F.one_hot(xs, num_classes=num_classes).float() # necessary to cast to float\n",
    "y_enc = F.one_hot(ys, num_classes=num_classes).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 27])\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(x_enc.shape)\n",
    "print(x_enc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A look at the first example, \".emma\"\n",
    "plt.imshow(x_enc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 27])\n",
      "tensor([[ 1.5161,  0.2518, -0.8134, -1.5261,  0.0199,  1.3987, -0.6605,  1.5468,\n",
      "         -1.4619,  1.3608,  0.1123, -0.7030, -0.0729, -0.3481,  0.4221, -0.3084,\n",
      "         -0.0906, -0.2959, -0.4079,  0.3095,  0.5801, -1.3938,  0.0429, -0.8249,\n",
      "          0.2482,  0.8125,  1.4743],\n",
      "        [-1.4768,  1.0658, -0.0847,  0.4306, -0.1614, -1.6583,  0.5668,  0.1087,\n",
      "         -0.5050,  0.5605,  1.4606, -0.0486,  0.4431, -0.8257,  1.6714,  0.3401,\n",
      "         -1.4550, -0.0719,  0.2468, -0.2624,  1.0457, -1.1185,  1.2681, -1.4233,\n",
      "          1.0308,  1.6871, -2.0817],\n",
      "        [-3.6138, -1.1762,  0.0248, -0.9052,  0.0333,  0.2542, -1.3890,  0.1746,\n",
      "         -1.8498, -1.1404, -0.5274,  2.2010, -0.5007, -0.2414, -0.0913, -0.1108,\n",
      "         -0.4225, -0.2936,  0.4787, -1.0636,  1.2015,  0.7441, -0.3593, -2.7035,\n",
      "          0.0993, -0.8035,  0.9771],\n",
      "        [-3.6138, -1.1762,  0.0248, -0.9052,  0.0333,  0.2542, -1.3890,  0.1746,\n",
      "         -1.8498, -1.1404, -0.5274,  2.2010, -0.5007, -0.2414, -0.0913, -0.1108,\n",
      "         -0.4225, -0.2936,  0.4787, -1.0636,  1.2015,  0.7441, -0.3593, -2.7035,\n",
      "          0.0993, -0.8035,  0.9771],\n",
      "        [-3.1554, -0.9987,  0.4605,  1.2044, -0.2920, -0.4695,  0.9029,  0.9308,\n",
      "          0.9137, -0.6340, -0.6091, -1.0254,  0.5976,  0.1487,  0.3892, -0.8174,\n",
      "         -1.0337,  0.2017,  0.0515, -0.0456, -0.8848,  0.0570,  0.8093, -0.3653,\n",
      "         -0.8681, -0.4622, -0.1539]])\n"
     ]
    }
   ],
   "source": [
    "# Testing a multiplication with a weight matrix\n",
    "W = torch.randn(num_classes, num_classes)\n",
    "logits = x_enc @ W\n",
    "\n",
    "# This should be 5x27:\n",
    "# (5, 27) @ (27, 27) -> (5, 27)\n",
    "print(logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2414)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at one logit\n",
    "logits[3, 13] # 4th example, 14th class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
      "         0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.2414,  0.0000,  0.0000,\n",
      "         0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
      "         0.0000,  0.0000,  0.0000])\n",
      "tensor(-0.2414)\n"
     ]
    }
   ],
   "source": [
    "# Reminder that each such logit is a sum of the products of the one-hot \n",
    "# encoded input and the corresponding row of the weight matrix\n",
    "print(x_enc[3] * W[:, 13])\n",
    "print((x_enc[3] * W[:, 13]).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.5545, 1.2863, 0.4433, 0.2174, 1.0201, 4.0501, 0.5166, 4.6966, 0.2318,\n",
       "         3.8992, 1.1189, 0.4951, 0.9297, 0.7061, 1.5252, 0.7346, 0.9134, 0.7439,\n",
       "         0.6650, 1.3627, 1.7862, 0.2481, 1.0439, 0.4383, 1.2818, 2.2535, 4.3680],\n",
       "        [0.2284, 2.9032, 0.9188, 1.5382, 0.8509, 0.1905, 1.7626, 1.1148, 0.6035,\n",
       "         1.7516, 4.3085, 0.9526, 1.5576, 0.4379, 5.3195, 1.4051, 0.2334, 0.9306,\n",
       "         1.2799, 0.7692, 2.8455, 0.3268, 3.5542, 0.2409, 2.8033, 5.4039, 0.1247],\n",
       "        [0.0269, 0.3085, 1.0252, 0.4045, 1.0339, 1.2894, 0.2493, 1.1907, 0.1573,\n",
       "         0.3197, 0.5901, 9.0343, 0.6061, 0.7856, 0.9128, 0.8952, 0.6554, 0.7455,\n",
       "         1.6139, 0.3452, 3.3252, 2.1045, 0.6982, 0.0670, 1.1044, 0.4478, 2.6566],\n",
       "        [0.0269, 0.3085, 1.0252, 0.4045, 1.0339, 1.2894, 0.2493, 1.1907, 0.1573,\n",
       "         0.3197, 0.5901, 9.0343, 0.6061, 0.7856, 0.9128, 0.8952, 0.6554, 0.7455,\n",
       "         1.6139, 0.3452, 3.3252, 2.1045, 0.6982, 0.0670, 1.1044, 0.4478, 2.6566],\n",
       "        [0.0426, 0.3683, 1.5849, 3.3347, 0.7467, 0.6253, 2.4666, 2.5366, 2.4935,\n",
       "         0.5305, 0.5438, 0.3587, 1.8178, 1.1603, 1.4758, 0.4416, 0.3557, 1.2235,\n",
       "         1.0528, 0.9554, 0.4128, 1.0586, 2.2463, 0.6940, 0.4197, 0.6299, 0.8574]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can consider the logits here \"log counts\" of the bigrams\n",
    "# We can see this more clearly by exponentiating the logits\n",
    "# This makes all numbers positive\n",
    "\n",
    "# This is equivalent to the N matrix from the statistical model notebook\n",
    "\n",
    "counts = logits.exp()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1097, 0.0310, 0.0107, 0.0052, 0.0246, 0.0975, 0.0124, 0.1131, 0.0056,\n",
       "         0.0939, 0.0269, 0.0119, 0.0224, 0.0170, 0.0367, 0.0177, 0.0220, 0.0179,\n",
       "         0.0160, 0.0328, 0.0430, 0.0060, 0.0251, 0.0106, 0.0309, 0.0543, 0.1052],\n",
       "        [0.0051, 0.0655, 0.0207, 0.0347, 0.0192, 0.0043, 0.0397, 0.0251, 0.0136,\n",
       "         0.0395, 0.0971, 0.0215, 0.0351, 0.0099, 0.1199, 0.0317, 0.0053, 0.0210,\n",
       "         0.0289, 0.0173, 0.0642, 0.0074, 0.0801, 0.0054, 0.0632, 0.1218, 0.0028],\n",
       "        [0.0008, 0.0095, 0.0315, 0.0124, 0.0317, 0.0396, 0.0076, 0.0365, 0.0048,\n",
       "         0.0098, 0.0181, 0.2772, 0.0186, 0.0241, 0.0280, 0.0275, 0.0201, 0.0229,\n",
       "         0.0495, 0.0106, 0.1020, 0.0646, 0.0214, 0.0021, 0.0339, 0.0137, 0.0815],\n",
       "        [0.0008, 0.0095, 0.0315, 0.0124, 0.0317, 0.0396, 0.0076, 0.0365, 0.0048,\n",
       "         0.0098, 0.0181, 0.2772, 0.0186, 0.0241, 0.0280, 0.0275, 0.0201, 0.0229,\n",
       "         0.0495, 0.0106, 0.1020, 0.0646, 0.0214, 0.0021, 0.0339, 0.0137, 0.0815],\n",
       "        [0.0014, 0.0121, 0.0521, 0.1096, 0.0245, 0.0205, 0.0810, 0.0833, 0.0819,\n",
       "         0.0174, 0.0179, 0.0118, 0.0597, 0.0381, 0.0485, 0.0145, 0.0117, 0.0402,\n",
       "         0.0346, 0.0314, 0.0136, 0.0348, 0.0738, 0.0228, 0.0138, 0.0207, 0.0282]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the log counts to probabilities summing to 1\n",
    "probs = counts / counts.sum(dim=1, keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary So Far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn(num_classes, num_classes, generator=g)\n",
    "\n",
    "x_enc = F.one_hot(xs, num_classes=num_classes).float() # input to the network: one-hot encodings\n",
    "y_enc = F.one_hot(ys, num_classes=num_classes).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the logits for all bigrams\n",
    "logits = x_enc @ W # predict log-counts\n",
    "\n",
    "# Softmax\n",
    "counts = torch.exp(logits) # counts, equivalent to N\n",
    "probs = counts / counts.sum(dim=1, keepdims=True) # probabilities for next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "bigram example 1: .e (indexes 0,5)\n",
      "input to the neural net: 0\n",
      "output probabilities from the neural net: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
      "label (actual next character): 5\n",
      "probability assigned by the net to the the correct character: 0.01228625513613224\n",
      "log likelihood: -4.399273872375488\n",
      "negative log likelihood: 4.399273872375488\n",
      "--------\n",
      "bigram example 2: em (indexes 5,13)\n",
      "input to the neural net: 5\n",
      "output probabilities from the neural net: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.018050700426101685\n",
      "log likelihood: -4.014570713043213\n",
      "negative log likelihood: 4.014570713043213\n",
      "--------\n",
      "bigram example 3: mm (indexes 13,13)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.026691533625125885\n",
      "log likelihood: -3.623408794403076\n",
      "negative log likelihood: 3.623408794403076\n",
      "--------\n",
      "bigram example 4: ma (indexes 13,1)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 1\n",
      "probability assigned by the net to the the correct character: 0.07367686182260513\n",
      "log likelihood: -2.6080665588378906\n",
      "negative log likelihood: 2.6080665588378906\n",
      "--------\n",
      "bigram example 5: a. (indexes 1,0)\n",
      "input to the neural net: 1\n",
      "output probabilities from the neural net: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "label (actual next character): 0\n",
      "probability assigned by the net to the the correct character: 0.014977526850998402\n",
      "log likelihood: -4.201204299926758\n",
      "negative log likelihood: 4.201204299926758\n",
      "=========\n",
      "average negative log likelihood, i.e. loss = 3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "# examining \".emma\"\n",
    "# Note high nll when the model assigns low likelihood to the correct character\n",
    "\n",
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "    # i-th bigram:\n",
    "    x = xs[i].item() # input character index\n",
    "    y = ys[i].item() # label character index\n",
    "    print('--------')\n",
    "    print(f'bigram example {i+1}: {i_to_s[x]}{i_to_s[y]} (indexes {x},{y})')\n",
    "    print('input to the neural net:', x)\n",
    "    print('output probabilities from the neural net:', probs[i])\n",
    "    print('label (actual next character):', y)\n",
    "    p = probs[i, y]\n",
    "    print('probability assigned by the net to the the correct character:', p.item())\n",
    "    logp = torch.log(p)\n",
    "    print('log likelihood:', logp.item())\n",
    "    nll = -logp\n",
    "    print('negative log likelihood:', nll.item())\n",
    "    nlls[i] = nll\n",
    "\n",
    "print('=========')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_enc = F.one_hot(xs, num_classes=num_classes).float()\n",
    "y_enc = F.one_hot(ys, num_classes=num_classes).float()\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn(num_classes, num_classes, generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0123, 0.0181, 0.0267, 0.0737, 0.0150])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To evaluate training, we can look at the probabilities of the characters we want to predict\n",
    "# In this case, we look at y for each character in .emma\n",
    "\n",
    "probs[torch.arange(5), ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7693)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Negative average log likelihood for this example:\n",
    "-probs[torch.arange(5), ys].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "\n",
    "logits = x_enc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(dim=1, keepdims=True)\n",
    "loss = -probs[torch.arange(len(ys)), ys].log().mean()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "W.grad = None # how to set gradient to zero in PyTorch\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update\n",
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7693049907684326\n",
      "3.7492127418518066\n"
     ]
    }
   ],
   "source": [
    "# Do another forward pass with the new data and compare with the previous loss\n",
    "print(loss.item())\n",
    "\n",
    "logits = x_enc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(dim=1, keepdims=True)\n",
    "loss = -probs[torch.arange(len(ys)), ys].log().mean()   \n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can then perform another backward pass, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training With a Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss = 3.7686190605163574\n",
      "step 1: loss = 3.3788068294525146\n",
      "step 2: loss = 3.161090850830078\n",
      "step 3: loss = 3.027186155319214\n",
      "step 4: loss = 2.9344842433929443\n",
      "step 5: loss = 2.8672313690185547\n",
      "step 6: loss = 2.816654682159424\n",
      "step 7: loss = 2.777146577835083\n",
      "step 8: loss = 2.745253801345825\n",
      "step 9: loss = 2.7188303470611572\n",
      "step 10: loss = 2.6965057849884033\n",
      "step 11: loss = 2.6773719787597656\n",
      "step 12: loss = 2.6608052253723145\n",
      "step 13: loss = 2.6463515758514404\n",
      "step 14: loss = 2.633665084838867\n",
      "step 15: loss = 2.622471570968628\n",
      "step 16: loss = 2.6125476360321045\n",
      "step 17: loss = 2.6037068367004395\n",
      "step 18: loss = 2.595794916152954\n",
      "step 19: loss = 2.5886809825897217\n",
      "step 20: loss = 2.582256317138672\n",
      "step 21: loss = 2.5764293670654297\n",
      "step 22: loss = 2.5711238384246826\n",
      "step 23: loss = 2.566272735595703\n",
      "step 24: loss = 2.5618226528167725\n",
      "step 25: loss = 2.5577263832092285\n",
      "step 26: loss = 2.5539445877075195\n",
      "step 27: loss = 2.550442695617676\n",
      "step 28: loss = 2.5471925735473633\n",
      "step 29: loss = 2.5441696643829346\n",
      "step 30: loss = 2.5413525104522705\n",
      "step 31: loss = 2.538721799850464\n",
      "step 32: loss = 2.536262035369873\n",
      "step 33: loss = 2.5339581966400146\n",
      "step 34: loss = 2.531797409057617\n",
      "step 35: loss = 2.5297679901123047\n",
      "step 36: loss = 2.527860164642334\n",
      "step 37: loss = 2.5260636806488037\n",
      "step 38: loss = 2.5243709087371826\n",
      "step 39: loss = 2.522773265838623\n",
      "step 40: loss = 2.521263837814331\n",
      "step 41: loss = 2.519836664199829\n",
      "step 42: loss = 2.5184855461120605\n",
      "step 43: loss = 2.517204999923706\n",
      "step 44: loss = 2.515990734100342\n",
      "step 45: loss = 2.5148372650146484\n",
      "step 46: loss = 2.5137410163879395\n",
      "step 47: loss = 2.512697696685791\n",
      "step 48: loss = 2.511704921722412\n",
      "step 49: loss = 2.5107579231262207\n",
      "step 50: loss = 2.509854793548584\n",
      "step 51: loss = 2.5089924335479736\n",
      "step 52: loss = 2.5081682205200195\n",
      "step 53: loss = 2.507380247116089\n",
      "step 54: loss = 2.5066258907318115\n",
      "step 55: loss = 2.5059030055999756\n",
      "step 56: loss = 2.5052108764648438\n",
      "step 57: loss = 2.5045459270477295\n",
      "step 58: loss = 2.503908157348633\n",
      "step 59: loss = 2.503295421600342\n",
      "step 60: loss = 2.502706289291382\n",
      "step 61: loss = 2.5021398067474365\n",
      "step 62: loss = 2.5015945434570312\n",
      "step 63: loss = 2.5010693073272705\n",
      "step 64: loss = 2.500562906265259\n",
      "step 65: loss = 2.500075578689575\n",
      "step 66: loss = 2.4996049404144287\n",
      "step 67: loss = 2.499150514602661\n",
      "step 68: loss = 2.4987120628356934\n",
      "step 69: loss = 2.49828839302063\n",
      "step 70: loss = 2.4978787899017334\n",
      "step 71: loss = 2.497483015060425\n",
      "step 72: loss = 2.4970998764038086\n",
      "step 73: loss = 2.4967286586761475\n",
      "step 74: loss = 2.496370315551758\n",
      "step 75: loss = 2.496022939682007\n",
      "step 76: loss = 2.4956860542297363\n",
      "step 77: loss = 2.4953596591949463\n",
      "step 78: loss = 2.4950432777404785\n",
      "step 79: loss = 2.4947361946105957\n",
      "step 80: loss = 2.494438648223877\n",
      "step 81: loss = 2.494149684906006\n",
      "step 82: loss = 2.4938690662384033\n",
      "step 83: loss = 2.4935967922210693\n",
      "step 84: loss = 2.4933321475982666\n",
      "step 85: loss = 2.493074893951416\n",
      "step 86: loss = 2.4928252696990967\n",
      "step 87: loss = 2.492582321166992\n",
      "step 88: loss = 2.4923462867736816\n",
      "step 89: loss = 2.492116928100586\n",
      "step 90: loss = 2.4918932914733887\n",
      "step 91: loss = 2.491675853729248\n",
      "step 92: loss = 2.491464376449585\n",
      "step 93: loss = 2.491258144378662\n",
      "step 94: loss = 2.491058111190796\n",
      "step 95: loss = 2.4908626079559326\n",
      "step 96: loss = 2.4906723499298096\n",
      "step 97: loss = 2.4904870986938477\n",
      "step 98: loss = 2.4903063774108887\n",
      "step 99: loss = 2.4901304244995117\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "num_steps = 100\n",
    "learning_rate = 50 # seems a large one works fine for this task\n",
    "reg_coefficient = 0.01 # regularization coefficient; high is like adding more counts to smoothing\n",
    "\n",
    "# Create full-size training set of bigrams (x, y)\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = s_to_i[ch1]\n",
    "        ix2 = s_to_i[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "x_enc = F.one_hot(xs, num_classes=num_classes).float()\n",
    "y_enc = F.one_hot(ys, num_classes=num_classes).float()\n",
    "\n",
    "# Randomly initialize the weight matrix\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn(num_classes, num_classes, generator=g, requires_grad=True)\n",
    "\n",
    "# Keep track of data for plotting\n",
    "losses = []\n",
    "\n",
    "# Training loop\n",
    "for i in range(num_steps):\n",
    "    # Forward pass\n",
    "    logits = x_enc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(dim=1, keepdims=True)\n",
    "    loss = -probs[torch.arange(len(ys)), ys].log().mean()\n",
    "    # Regularization\n",
    "    loss += reg_coefficient * (W ** 2).mean()\n",
    "    losses.append(loss.item())\n",
    "    print(f'step {i}: loss = {loss.item()}')\n",
    "\n",
    "    # Backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    W.data += -learning_rate * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cfay.\n",
      "a.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        x_enc = F.one_hot(torch.tensor([ix]), num_classes=num_classes).float()\n",
    "        logits = x_enc @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(i_to_s[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makemore-lm-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
